{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176582dc-8392-489e-a0de-b8ae2653a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# Load ESM-2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff325a5-eb72-4747-b7da-cb4eb171b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pseudo_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89cdef0-1dd9-4f9d-8c7c-57a9c5309cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdecf1e-d723-436e-b43f-a67b48810218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94dd7d6d-17f5-41b3-a197-62b4499b1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "             \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\",\n",
    "             \"KALTARQQEVFDLIRDISQTGMPPTRAEIAKALTFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\",\n",
    "             \"KAISQ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d77df-13a8-417b-b414-08a9811c5d57",
   "metadata": {},
   "source": [
    "### ProtBERT Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599845cd-6220-4dbb-83a0-acc9b89da53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import re\n",
    "import protbert\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "def prepare_sequence(sequences):\n",
    "    sequences = [add_space(sequence) for sequence in sequences]\n",
    "    return sequences\n",
    "\n",
    "def add_space(row):\n",
    "    if not isinstance(row, float):\n",
    "        row = \" \".join(row)\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc4ace9b-daeb-4373-a1fa-98b15b463958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 3/4 [01:09<00:23, 23.15s/it]\n"
     ]
    }
   ],
   "source": [
    "sequences = prepare_sequence(sequences)\n",
    "\n",
    "for sequence,_ in zip(enumerate(sequences), tqdm(range(len(sequences)))):\n",
    "            if not isinstance(sequence[1], float):\n",
    "                tokenized_sequences = tokenizer(sequence[1], return_tensors= 'pt') #return tensors using pytorch\n",
    "                output = model(**tokenized_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a117a-7079-4d61-aa6d-058857964f28",
   "metadata": {},
   "source": [
    "### ProtBERT likelihood computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1fdf850-1ef2-4ae1-b1df-167555fae5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### attempt 1\n",
    "\n",
    "mask_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "unmasker = pipeline('fill-mask', model=mask_model, tokenizer=tokenizer, top_k = 21)\n",
    "\n",
    "def prepare_sequence_MLM(seq_tokens, pos):\n",
    "    x = seq_tokens.copy()\n",
    "    x[pos] = \"[MASK]\"\n",
    "    return ' '.join(x)\n",
    "\n",
    "probs = []\n",
    "for sequence in tqdm(sequences):\n",
    "    probs_seq = []\n",
    "    seq_tokens = list(sequence)\n",
    "    for pos in tqdm(range(len(sequence))):\n",
    "        prep_seq = prepare_sequence_MLM(seq_tokens, pos)\n",
    "        scores = unmasker(prep_seq)\n",
    "        scores_dict = {dict[\"token_str\"]:dict[\"score\"] for dict in scores}\n",
    "        probs_seq.append(scores_dict)\n",
    "    probs.append(pd.DataFrame(probs_seq))\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9832587-4e47-4f10-86ec-98acc2daa497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "### attempt 2\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mask_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "probs = []\n",
    "\n",
    "                        \n",
    "for sequence in tqdm(sequences):\n",
    "    seq_tokens = ' '.join(list(sequence))\n",
    "    seq_tokens = tokenizer(seq_tokens, return_tensors='pt').to(device)\n",
    "    logits = mask_model(**seq_tokens).logits[0].cpu().detach().numpy()\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = tokenizer.vocab)\n",
    "    df = df.iloc[:,5:-5]\n",
    "    df = df.loc[:, df.columns.isin([\"U\",\"Z\",\"O\",\"B\",\"X\"]) == False]\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fce2863b-6228-46f0-bafc-76857fb6bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12570/12570 [02:11<00:00, 95.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_pseudo_likelihood(probs, sequences):\n",
    "    probs_all = []\n",
    "    for i,sequence_probs in enumerate(tqdm(probs)):\n",
    "        wt_probs_full = []\n",
    "        for pos in range(sequence_probs.shape[0]):\n",
    "\n",
    "            wt_j = sequences[i][pos]\n",
    "            #Can comment if PLM gives probabilities also for gaps\n",
    "            if wt_j == \"-\" or wt_j ==\"*\":\n",
    "                continue\n",
    "            wt_prob = sequence_probs.iloc[pos,:][wt_j]\n",
    "            wt_probs_full.append(np.log(wt_prob))\n",
    "        probs_all.append(np.average(wt_probs_full))\n",
    "    return probs_all\n",
    "    \n",
    "pseudo = get_pseudo_likelihood(probs, list(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3aa1b054-597f-4ac3-a76d-b5dc7790208b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1878769,\n",
       " -0.18736279,\n",
       " -0.19010155,\n",
       " -0.27160725,\n",
       " -0.20921014,\n",
       " -0.17248185,\n",
       " -0.16602251,\n",
       " -0.18929744,\n",
       " -0.21467482,\n",
       " -0.16016108,\n",
       " -0.22486228,\n",
       " -0.17152075,\n",
       " -0.17114305,\n",
       " -0.20624895,\n",
       " -0.18542437,\n",
       " -0.28184122,\n",
       " -0.20921014,\n",
       " -0.13305043,\n",
       " -0.15144818,\n",
       " -0.16825531,\n",
       " -0.15589093,\n",
       " -0.16131198,\n",
       " -0.14519814,\n",
       " -0.15034878,\n",
       " -0.18184976,\n",
       " -0.19896989,\n",
       " -0.13709423,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.20334244,\n",
       " -0.19303684,\n",
       " -0.17274092,\n",
       " -0.13191147,\n",
       " -0.16480674,\n",
       " -0.23642138,\n",
       " -0.1905893,\n",
       " -0.1584998,\n",
       " -0.15452017,\n",
       " -0.16783366,\n",
       " -0.16203225,\n",
       " -0.17442696,\n",
       " -0.15610264,\n",
       " -0.17780803,\n",
       " -0.19270894,\n",
       " -0.17460558,\n",
       " -0.16304336,\n",
       " -0.15370403,\n",
       " -0.15483429,\n",
       " -0.19534646,\n",
       " -0.16098097,\n",
       " -0.20988026,\n",
       " -0.2032551,\n",
       " -0.13420674,\n",
       " -0.19551572,\n",
       " -0.12234064,\n",
       " -0.15980375,\n",
       " -0.2293673,\n",
       " -0.18524839,\n",
       " -0.17762665,\n",
       " -0.13517477,\n",
       " -0.21786273,\n",
       " -0.23571877,\n",
       " -0.15510426,\n",
       " -0.23508891,\n",
       " -0.26100925,\n",
       " -0.16012886,\n",
       " -0.23339401,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.18607841,\n",
       " -0.1865574,\n",
       " -0.15395415,\n",
       " -0.20699522,\n",
       " -0.12795366,\n",
       " -0.21065581,\n",
       " -0.15804395,\n",
       " -0.17340504,\n",
       " -0.19324096,\n",
       " -0.18484214,\n",
       " -0.2279269,\n",
       " -0.21667214,\n",
       " -0.11611439,\n",
       " -0.12150139,\n",
       " -0.13698089,\n",
       " -0.24454285,\n",
       " -0.21192424,\n",
       " -0.15616912,\n",
       " -0.23033732,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.15267055,\n",
       " -0.21181835,\n",
       " -0.20210609,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.19720772,\n",
       " -0.12973468,\n",
       " -0.17967333,\n",
       " -0.27108625,\n",
       " -0.20453456,\n",
       " -0.12893444,\n",
       " -0.2481234,\n",
       " -0.19435397,\n",
       " -0.20631902,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.21066627,\n",
       " -0.21238424,\n",
       " -0.22520326,\n",
       " -0.15717648,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.17351791,\n",
       " -0.15267055,\n",
       " -0.2003338,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.22408058,\n",
       " -0.23866913,\n",
       " -0.23250721,\n",
       " -0.20782065,\n",
       " -0.13410741,\n",
       " -0.15061346,\n",
       " -0.19748913,\n",
       " -0.17578456,\n",
       " -0.19212297,\n",
       " -0.21791111,\n",
       " -0.17833371,\n",
       " -0.17722413,\n",
       " -0.12234064,\n",
       " -0.1864007,\n",
       " -0.20119017,\n",
       " -0.19550946,\n",
       " -0.19097584,\n",
       " -0.21746437,\n",
       " -0.23097865,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.27069306,\n",
       " -0.21964437,\n",
       " -0.21374926,\n",
       " -0.13196969,\n",
       " -0.17475331,\n",
       " -0.12752755,\n",
       " -0.23866913,\n",
       " -0.21786273,\n",
       " -0.16332076,\n",
       " -0.20353884,\n",
       " -0.17912671,\n",
       " -0.16806804,\n",
       " -0.19926995,\n",
       " -0.17862026,\n",
       " -0.18722717,\n",
       " -0.17248185,\n",
       " -0.23416544,\n",
       " -0.18014091,\n",
       " -0.2180712,\n",
       " -0.22360812,\n",
       " -0.19015367,\n",
       " -0.20653673,\n",
       " -0.2481234,\n",
       " -0.19729088,\n",
       " -0.13858376,\n",
       " -0.15646191,\n",
       " -0.19501156,\n",
       " -0.17751388,\n",
       " -0.20321383,\n",
       " -0.173088,\n",
       " -0.12604652,\n",
       " -0.2042667,\n",
       " -0.1492326,\n",
       " -0.14617822,\n",
       " -0.23642138,\n",
       " -0.19483228,\n",
       " -0.23642138,\n",
       " -0.20956427,\n",
       " -0.19935524,\n",
       " -0.17751388,\n",
       " -0.14185426,\n",
       " -0.22868699,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.14625241,\n",
       " -0.20770408,\n",
       " -0.18037291,\n",
       " -0.1728345,\n",
       " -0.20613164,\n",
       " -0.23642138,\n",
       " -0.31604487,\n",
       " -0.21656625,\n",
       " -0.19935524,\n",
       " -0.17751388,\n",
       " -0.18552539,\n",
       " -0.20434953,\n",
       " -0.28174862,\n",
       " -0.20921014,\n",
       " -0.16413555,\n",
       " -0.14887309,\n",
       " -0.17246394,\n",
       " -0.2527546,\n",
       " -0.18960315,\n",
       " -0.15476352,\n",
       " -0.19541118,\n",
       " -0.16960098,\n",
       " -0.20106481,\n",
       " -0.24537492,\n",
       " -0.20988026,\n",
       " -0.16098097,\n",
       " -0.16332076,\n",
       " -0.16326904,\n",
       " -0.21340263,\n",
       " -0.16338453,\n",
       " -0.21246418,\n",
       " -0.1618257,\n",
       " -0.21746437,\n",
       " -0.23097865,\n",
       " -0.16034368,\n",
       " -0.21787612,\n",
       " -0.15740019,\n",
       " -0.19283386,\n",
       " -0.25463098,\n",
       " -0.22201422,\n",
       " -0.18723856,\n",
       " -0.23063558,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.20608605,\n",
       " -0.2149033,\n",
       " -0.14242467,\n",
       " -0.21654308,\n",
       " -0.2660534,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.1864816,\n",
       " -0.13930006,\n",
       " -0.18719247,\n",
       " -0.18184976,\n",
       " -0.19653958,\n",
       " -0.21667214,\n",
       " -0.22748531,\n",
       " -0.23765987,\n",
       " -0.14639409,\n",
       " -0.16922249,\n",
       " -0.26811627,\n",
       " -0.12513147,\n",
       " -0.1524058,\n",
       " -0.2041461,\n",
       " -0.18337186,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.19483607,\n",
       " -0.1810162,\n",
       " -0.22408058,\n",
       " -0.22859251,\n",
       " -0.23213944,\n",
       " -0.23642138,\n",
       " -0.22247674,\n",
       " -0.22408058,\n",
       " -0.23438829,\n",
       " -0.21039465,\n",
       " -0.22890218,\n",
       " -0.2129386,\n",
       " -0.19453736,\n",
       " -0.20844829,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.15900765,\n",
       " -0.18184976,\n",
       " -0.15585434,\n",
       " -0.23888312,\n",
       " -0.20631902,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.21786273,\n",
       " -0.23644738,\n",
       " -0.21605982,\n",
       " -0.13786733,\n",
       " -0.25220916,\n",
       " -0.15465651,\n",
       " -0.17856616,\n",
       " -0.2111434,\n",
       " -0.17392479,\n",
       " -0.24630386,\n",
       " -0.17965578,\n",
       " -0.18624097,\n",
       " -0.21654461,\n",
       " -0.19657858,\n",
       " -0.20079385,\n",
       " -0.1600592,\n",
       " -0.23117785,\n",
       " -0.15034878,\n",
       " -0.19379741,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.1683671,\n",
       " -0.20731728,\n",
       " -0.20433931,\n",
       " -0.15407608,\n",
       " -0.16710728,\n",
       " -0.16199625,\n",
       " -0.13840215,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.16806804,\n",
       " -0.19926995,\n",
       " -0.21746437,\n",
       " -0.24204642,\n",
       " -0.15905419,\n",
       " -0.1549341,\n",
       " -0.17505886,\n",
       " -0.22794835,\n",
       " -0.1931078,\n",
       " -0.17292714,\n",
       " -0.23640324,\n",
       " -0.15034878,\n",
       " -0.18184976,\n",
       " -0.21031088,\n",
       " -0.20398545,\n",
       " -0.20254497,\n",
       " -0.1508071,\n",
       " -0.18220839,\n",
       " -0.13902919,\n",
       " -0.16990788,\n",
       " -0.24077041,\n",
       " -0.21740903,\n",
       " -0.13689394,\n",
       " -0.21902402,\n",
       " -0.16688211,\n",
       " -0.21724874,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.19907631,\n",
       " -0.16731353,\n",
       " -0.17637815,\n",
       " -0.21667214,\n",
       " -0.15250711,\n",
       " -0.20936117,\n",
       " -0.1683671,\n",
       " -0.20731728,\n",
       " -0.12663294,\n",
       " -0.16415353,\n",
       " -0.18420216,\n",
       " -0.16778144,\n",
       " -0.16098097,\n",
       " -0.224701,\n",
       " -0.23492917,\n",
       " -0.23091803,\n",
       " -0.17727086,\n",
       " -0.20306551,\n",
       " -0.2212842,\n",
       " -0.16475813,\n",
       " -0.19593504,\n",
       " -0.1900008,\n",
       " -0.16083464,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.21283862,\n",
       " -0.21781659,\n",
       " -0.13607384,\n",
       " -0.22748531,\n",
       " -0.23671682,\n",
       " -0.24192551,\n",
       " -0.19079696,\n",
       " -0.19616371,\n",
       " -0.18854515,\n",
       " -0.13896184,\n",
       " -0.15985128,\n",
       " -0.20874393,\n",
       " -0.16399626,\n",
       " -0.1673715,\n",
       " -0.18804553,\n",
       " -0.13930006,\n",
       " -0.15350547,\n",
       " -0.19435397,\n",
       " -0.14952269,\n",
       " -0.2492843,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.18418644,\n",
       " -0.17473172,\n",
       " -0.16423753,\n",
       " -0.16603462,\n",
       " -0.14746857,\n",
       " -0.13930006,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.19690932,\n",
       " -0.17590316,\n",
       " -0.15061346,\n",
       " -0.19748913,\n",
       " -0.21917622,\n",
       " -0.16809136,\n",
       " -0.17116958,\n",
       " -0.16977948,\n",
       " -0.20054823,\n",
       " -0.21370918,\n",
       " -0.22452797,\n",
       " -0.19452722,\n",
       " -0.2869418,\n",
       " -0.21656625,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.19830903,\n",
       " -0.19359894,\n",
       " -0.14186189,\n",
       " -0.1412289,\n",
       " -0.14205901,\n",
       " -0.16003977,\n",
       " -0.21173951,\n",
       " -0.17040385,\n",
       " -0.21656625,\n",
       " -0.27889383,\n",
       " -0.22394301,\n",
       " -0.17321202,\n",
       " -0.1662033,\n",
       " -0.22071122,\n",
       " -0.19926995,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.18466659,\n",
       " -0.19546579,\n",
       " -0.19101651,\n",
       " -0.2779998,\n",
       " -0.21656625,\n",
       " -0.16990788,\n",
       " -0.22427829,\n",
       " -0.24429676,\n",
       " -0.25863454,\n",
       " -0.21045233,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.23098777,\n",
       " -0.14562342,\n",
       " -0.11922905,\n",
       " -0.20398545,\n",
       " -0.21776728,\n",
       " -0.19097584,\n",
       " -0.1637635,\n",
       " -0.16852522,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.20957115,\n",
       " -0.23642138,\n",
       " -0.23438829,\n",
       " -0.22408058,\n",
       " -0.16540611,\n",
       " -0.2410829,\n",
       " -0.29014164,\n",
       " -0.18807167,\n",
       " -0.20325628,\n",
       " -0.12459816,\n",
       " -0.19707839,\n",
       " -0.2104649,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.16495213,\n",
       " -0.20865808,\n",
       " -0.18621732,\n",
       " -0.21278909,\n",
       " -0.18256603,\n",
       " -0.18196331,\n",
       " -0.27528772,\n",
       " -0.19554643,\n",
       " -0.159028,\n",
       " -0.20479167,\n",
       " -0.19148621,\n",
       " -0.18850417,\n",
       " -0.20401825,\n",
       " -0.1682645,\n",
       " -0.19630307,\n",
       " -0.1974074,\n",
       " -0.21007146,\n",
       " -0.24231823,\n",
       " -0.2214665,\n",
       " -0.18214121,\n",
       " -0.15447924,\n",
       " -0.26100925,\n",
       " -0.16012886,\n",
       " -0.16783366,\n",
       " -0.1613215,\n",
       " -0.13430907,\n",
       " -0.18368275,\n",
       " -0.16425377,\n",
       " -0.20210609,\n",
       " -0.20382278,\n",
       " -0.17557144,\n",
       " -0.18849726,\n",
       " -0.15509002,\n",
       " -0.18471801,\n",
       " -0.20865808,\n",
       " -0.18621732,\n",
       " -0.20254497,\n",
       " -0.13847922,\n",
       " -0.121571556,\n",
       " -0.13858376,\n",
       " -0.20101935,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.22859655,\n",
       " -0.21883667,\n",
       " -0.23385999,\n",
       " -0.19719069,\n",
       " -0.18103309,\n",
       " -0.1389512,\n",
       " -0.23119137,\n",
       " -0.21829228,\n",
       " -0.17206345,\n",
       " -0.19902672,\n",
       " -0.20846689,\n",
       " -0.18557568,\n",
       " -0.18252625,\n",
       " -0.1524058,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.2779998,\n",
       " -0.21656625,\n",
       " -0.1676809,\n",
       " -0.2257838,\n",
       " -0.19830903,\n",
       " -0.19359894,\n",
       " -0.22748531,\n",
       " -0.23478001,\n",
       " -0.27237013,\n",
       " -0.13893946,\n",
       " -0.24685928,\n",
       " -0.1829062,\n",
       " -0.18011728,\n",
       " -0.17930272,\n",
       " -0.18928254,\n",
       " -0.23754412,\n",
       " -0.2186667,\n",
       " -0.13553444,\n",
       " -0.2314806,\n",
       " -0.2551301,\n",
       " -0.15034878,\n",
       " -0.18184976,\n",
       " -0.12985484,\n",
       " -0.16835362,\n",
       " -0.22408058,\n",
       " -0.19458179,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.24132487,\n",
       " -0.12513147,\n",
       " -0.19566377,\n",
       " -0.13196969,\n",
       " -0.19228502,\n",
       " -0.18140315,\n",
       " -0.28084508,\n",
       " -0.26360053,\n",
       " -0.20394196,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.13430907,\n",
       " -0.1967497,\n",
       " -0.28184122,\n",
       " -0.20921014,\n",
       " -0.12795366,\n",
       " -0.22006111,\n",
       " -0.16483214,\n",
       " -0.17565186,\n",
       " -0.23545793,\n",
       " -0.20066133,\n",
       " -0.28064585,\n",
       " -0.20921014,\n",
       " -0.20631902,\n",
       " -0.19010155,\n",
       " -0.29708934,\n",
       " -0.20921014,\n",
       " -0.2294199,\n",
       " -0.17917375,\n",
       " -0.1683671,\n",
       " -0.14715707,\n",
       " -0.1773834,\n",
       " -0.15359378,\n",
       " -0.19663978,\n",
       " -0.16194189,\n",
       " -0.21592216,\n",
       " -0.23642138,\n",
       " -0.21850926,\n",
       " -0.13440035,\n",
       " -0.21656625,\n",
       " -0.28064585,\n",
       " -0.15660867,\n",
       " -0.17912671,\n",
       " -0.1338915,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.15387519,\n",
       " -0.28709567,\n",
       " -0.17556392,\n",
       " -0.17891267,\n",
       " -0.20656492,\n",
       " -0.16365668,\n",
       " -0.12663294,\n",
       " -0.18420216,\n",
       " -0.16778144,\n",
       " -0.1752727,\n",
       " -0.20005529,\n",
       " -0.12868637,\n",
       " -0.14635782,\n",
       " -0.23423173,\n",
       " -0.21667214,\n",
       " -0.23339401,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.1534316,\n",
       " -0.17924687,\n",
       " -0.12551212,\n",
       " -0.20955585,\n",
       " -0.20758355,\n",
       " -0.17733471,\n",
       " -0.18686102,\n",
       " -0.12795366,\n",
       " -0.24329339,\n",
       " -0.24407227,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.14982721,\n",
       " -0.21803334,\n",
       " -0.13014379,\n",
       " -0.18875833,\n",
       " -0.23866913,\n",
       " -0.22408058,\n",
       " -0.22748531,\n",
       " -0.23478001,\n",
       " -0.19461079,\n",
       " -0.18461621,\n",
       " -0.1937676,\n",
       " -0.20970887,\n",
       " -0.17570068,\n",
       " -0.1282409,\n",
       " -0.13215213,\n",
       " -0.24844101,\n",
       " -0.21656625,\n",
       " -0.28064585,\n",
       " -0.26024255,\n",
       " -0.25007522,\n",
       " -0.18827313,\n",
       " -0.2729621,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.21786273,\n",
       " -0.23620215,\n",
       " -0.19900665,\n",
       " -0.17638256,\n",
       " -0.2400179,\n",
       " -0.22846699,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.21637335,\n",
       " -0.24516906,\n",
       " -0.13062161,\n",
       " -0.2022332,\n",
       " -0.2862625,\n",
       " -0.27593,\n",
       " -0.22748531,\n",
       " -0.23478001,\n",
       " -0.16990788,\n",
       " -0.13786733,\n",
       " -0.20242234,\n",
       " -0.16098097,\n",
       " -0.20988026,\n",
       " -0.18304901,\n",
       " -0.16466875,\n",
       " -0.25040174,\n",
       " -0.2511241,\n",
       " -0.2188465,\n",
       " -0.1866367,\n",
       " -0.21072707,\n",
       " -0.17718692,\n",
       " -0.31011605,\n",
       " -0.20921014,\n",
       " -0.16833395,\n",
       " -0.13453642,\n",
       " -0.18379883,\n",
       " -0.18736279,\n",
       " -0.17683025,\n",
       " -0.22118495,\n",
       " -0.24958463,\n",
       " -0.2345248,\n",
       " -0.18252625,\n",
       " -0.12315111,\n",
       " -0.21064286,\n",
       " -0.2636804,\n",
       " -0.17939448,\n",
       " -0.15364832,\n",
       " -0.20933175,\n",
       " -0.18354627,\n",
       " -0.17576948,\n",
       " -0.18146561,\n",
       " -0.15980375,\n",
       " -0.2293673,\n",
       " -0.16009165,\n",
       " -0.21638726,\n",
       " -0.2847775,\n",
       " -0.18155928,\n",
       " -0.18183106,\n",
       " -0.20426267,\n",
       " -0.2079424,\n",
       " -0.15536478,\n",
       " -0.23213944,\n",
       " -0.23571877,\n",
       " -0.18253742,\n",
       " -0.27141196,\n",
       " -0.2314806,\n",
       " -0.19729088,\n",
       " -0.15646191,\n",
       " -0.121571556,\n",
       " -0.17777449,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.15395415,\n",
       " -0.20699522,\n",
       " -0.15900765,\n",
       " -0.18184976,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.16294578,\n",
       " -0.22145854,\n",
       " -0.21786273,\n",
       " -0.23438829,\n",
       " -0.15395415,\n",
       " -0.20699522,\n",
       " -0.1376503,\n",
       " -0.17536387,\n",
       " -0.14765055,\n",
       " -0.15053698,\n",
       " -0.20633869,\n",
       " -0.19097584,\n",
       " -0.13410741,\n",
       " -0.20057112,\n",
       " -0.22318815,\n",
       " -0.17389959,\n",
       " -0.27026448,\n",
       " -0.15561578,\n",
       " -0.12513147,\n",
       " -0.17473172,\n",
       " -0.19751093,\n",
       " -0.2492843,\n",
       " -0.14952269,\n",
       " -0.19435397,\n",
       " -0.16603394,\n",
       " -0.16758814,\n",
       " -0.21786273,\n",
       " -0.23665805,\n",
       " -0.22408058,\n",
       " -0.14141129,\n",
       " -0.23571877,\n",
       " -0.17970262,\n",
       " -0.21612385,\n",
       " -0.17989032,\n",
       " -0.20291856,\n",
       " -0.13196969,\n",
       " -0.28184122,\n",
       " -0.20921014,\n",
       " -0.15739147,\n",
       " -0.18028553,\n",
       " -0.20002457,\n",
       " -0.19500071,\n",
       " -0.17747831,\n",
       " -0.19719069,\n",
       " -0.29342005,\n",
       " -0.19523405,\n",
       " -0.19218086,\n",
       " -0.19903344,\n",
       " -0.21394895,\n",
       " -0.19128698,\n",
       " -0.15061346,\n",
       " -0.19748913,\n",
       " -0.15925148,\n",
       " -0.27373776,\n",
       " -0.3150893,\n",
       " -0.21656625,\n",
       " -0.20865808,\n",
       " -0.18621732,\n",
       " -0.20812228,\n",
       " -0.18621732,\n",
       " -0.20325628,\n",
       " -0.12459816,\n",
       " -0.17683025,\n",
       " -0.22118495,\n",
       " -0.20189692,\n",
       " -0.22188784,\n",
       " -0.16601695,\n",
       " -0.20638789,\n",
       " -0.21758625,\n",
       " -0.18077607,\n",
       " -0.18343264,\n",
       " -0.13930006,\n",
       " -0.14936587,\n",
       " -0.19418666,\n",
       " -0.18957013,\n",
       " -0.19402269,\n",
       " -0.18471801,\n",
       " -0.16602089,\n",
       " -0.23296425,\n",
       " -0.17248185,\n",
       " -0.17060758,\n",
       " -0.21656625,\n",
       " -0.28064585,\n",
       " -0.16020478,\n",
       " -0.2086815,\n",
       " -0.23200631,\n",
       " -0.16990788,\n",
       " -0.20210609,\n",
       " -0.13410741,\n",
       " -0.18541513,\n",
       " -0.20210609,\n",
       " -0.16900755,\n",
       " -0.17547624,\n",
       " -0.18543997,\n",
       " -0.1847033,\n",
       " -0.23999749,\n",
       " -0.21792306,\n",
       " -0.1973051,\n",
       " -0.15458502,\n",
       " -0.17194885,\n",
       " -0.20576034,\n",
       " -0.21238586,\n",
       " -0.17998354,\n",
       " -0.28174862,\n",
       " -0.21656625,\n",
       " -0.18542437,\n",
       " -0.15565774,\n",
       " -0.21032363,\n",
       " -0.25632256,\n",
       " -0.16155963,\n",
       " -0.14494702,\n",
       " -0.2144518,\n",
       " -0.15625705,\n",
       " -0.21340263,\n",
       " -0.16338453,\n",
       " -0.122798964,\n",
       " -0.15900765,\n",
       " -0.18184976,\n",
       " -0.20043384,\n",
       " -0.13119984,\n",
       " -0.23438829,\n",
       " -0.22408058,\n",
       " -0.2174202,\n",
       " -0.29940897,\n",
       " -0.16542174,\n",
       " -0.2186854,\n",
       " -0.13690305,\n",
       " -0.13761653,\n",
       " -0.21264197,\n",
       " -0.19913293,\n",
       " -0.15820181,\n",
       " -0.18813583,\n",
       " -0.23535536,\n",
       " -0.19063494,\n",
       " -0.27258223,\n",
       " -0.20153534,\n",
       " -0.16449887,\n",
       " -0.1878248,\n",
       " -0.16598144,\n",
       " -0.23020008,\n",
       " -0.18243635,\n",
       " -0.21801381,\n",
       " -0.21764007,\n",
       " -0.15683392,\n",
       " -0.16828121,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.23642138,\n",
       " -0.1723712,\n",
       " -0.21702045,\n",
       " -0.12513147,\n",
       " -0.17019673,\n",
       " -0.17312048,\n",
       " -0.19968912,\n",
       " -0.20284174,\n",
       " -0.15769388,\n",
       " -0.12881346,\n",
       " -0.16951697,\n",
       " -0.26180926,\n",
       " -0.1698749,\n",
       " -0.21786273,\n",
       " -0.23877406,\n",
       " -0.1683671,\n",
       " -0.24371919,\n",
       " -0.18039803,\n",
       " -0.20912977,\n",
       " -0.17970262,\n",
       " -0.15509002,\n",
       " -0.18471801,\n",
       " -0.26520485,\n",
       " -0.15313078,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.22204232,\n",
       " -0.2847775,\n",
       " -0.18155928,\n",
       " -0.16601695,\n",
       " -0.21765392,\n",
       " -0.16982464,\n",
       " -0.20126794,\n",
       " -0.22748531,\n",
       " -0.22745569,\n",
       " -0.21786273,\n",
       " -0.23515871,\n",
       " -0.13761653,\n",
       " -0.21264197,\n",
       " -0.17856191,\n",
       " -0.1545829,\n",
       " -0.20438135,\n",
       " -0.20398545,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.21786273,\n",
       " -0.22720699,\n",
       " -0.21065585,\n",
       " -0.22847742,\n",
       " -0.23233716,\n",
       " -0.22955641,\n",
       " -0.1683671,\n",
       " -0.20673575,\n",
       " -0.28174862,\n",
       " -0.21656625,\n",
       " -0.16188124,\n",
       " -0.11496564,\n",
       " -0.19536877,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.23642138,\n",
       " -0.23767775,\n",
       " -0.16273113,\n",
       " -0.13890453,\n",
       " -0.16413555,\n",
       " -0.28064585,\n",
       " -0.21656625,\n",
       " -0.13840215,\n",
       " -0.23514017,\n",
       " -0.20144995,\n",
       " -0.28118977,\n",
       " -0.22305089,\n",
       " -0.16265535,\n",
       " -0.16208021,\n",
       " -0.2642926,\n",
       " -0.19723375,\n",
       " -0.20501366,\n",
       " -0.17970262,\n",
       " -0.17020816,\n",
       " -0.18256603,\n",
       " -0.19567552,\n",
       " -0.16532612,\n",
       " -0.1878248,\n",
       " -0.16598144,\n",
       " -0.17568891,\n",
       " -0.23642138,\n",
       " -0.19740167,\n",
       " -0.16601695,\n",
       " -0.18928254,\n",
       " -0.17930272,\n",
       " -0.21786273,\n",
       " -0.23544867,\n",
       " -0.144918,\n",
       " -0.1699317,\n",
       " -0.16076012,\n",
       " -0.17660148,\n",
       " -0.16392496,\n",
       " -0.22748531,\n",
       " -0.22754624,\n",
       " -0.15395415,\n",
       " -0.20699522,\n",
       " -0.14887309,\n",
       " -0.16413555,\n",
       " -0.16034368,\n",
       " -0.21787612,\n",
       " -0.1779074,\n",
       " -0.20016924,\n",
       " -0.22408058,\n",
       " -0.1864816,\n",
       " -0.13930006,\n",
       " -0.17893681,\n",
       " -0.18386893,\n",
       " -0.16205399,\n",
       " -0.16355301,\n",
       " -0.15087305,\n",
       " -0.16466781,\n",
       " -0.19034284,\n",
       " -0.18893923,\n",
       " -0.17206345,\n",
       " -0.20881392,\n",
       " -0.1506992,\n",
       " -0.20627896,\n",
       " -0.23934811,\n",
       " -0.17040385,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.21458061,\n",
       " -0.17970262,\n",
       " -0.13786733,\n",
       " -0.23257703,\n",
       " -0.21276161,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc6ea0-8bf9-4a4b-b7dd-5d94280eb990",
   "metadata": {},
   "source": [
    "### Antiberty likelihood Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bca5550-ef59-4775-99b1-ab084f06735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.34it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mantiberty\u001b[39;00m\n\u001b[0;32m      3\u001b[0m antiberty \u001b[38;5;241m=\u001b[39m antiberty\u001b[38;5;241m.\u001b[39mAntiBERTyRunner()\n\u001b[1;32m----> 5\u001b[0m pll \u001b[38;5;241m=\u001b[39m \u001b[43mantiberty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpseudo_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\antiberty\\AntiBERTyRunner.py:269\u001b[0m, in \u001b[0;36mAntiBERTyRunner.pseudo_log_likelihood\u001b[1;34m(self, sequences, batch_size)\u001b[0m\n\u001b[0;32m    264\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiagonal(logits, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    265\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mlist\u001b[39m(s)),\n\u001b[0;32m    267\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    268\u001b[0m )[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 269\u001b[0m nll \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m pll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnll\n\u001b[0;32m    276\u001b[0m plls\u001b[38;5;241m.\u001b[39mappend(pll)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA__nll_loss2d_forward)"
     ]
    }
   ],
   "source": [
    "import antiberty\n",
    "\n",
    "antiberty = antiberty.AntiBERTyRunner()\n",
    "\n",
    "pll = antiberty.pseudo_log_likelihood(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe27d6a-ee6f-4062-920e-8af97ef44147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antiberty.tokenizer.encode(\"M M M\", return_tensors=\"pt\").device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e557dad-9ea2-4a0c-8f2b-cf4e4f9d4f4c",
   "metadata": {},
   "source": [
    "### Ablang likelihood Computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9710cc-a7d5-4cd6-afd7-3c29b42938d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'pretrained' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mablang\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m heavy_ablang \u001b[38;5;241m=\u001b[39m \u001b[43mablang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheavy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m heavy_ablang\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'pretrained' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import ablang\n",
    "\n",
    "heavy_ablang = ablang.pretrained(\"heavy\")\n",
    "heavy_ablang.freeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a01e4c69-3416-455d-90ee-8764d02051f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "for sequence in tqdm(sequences):\n",
    "    logits = heavy_ablang(sequence, mode=\"likelihood\")[0]\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = list(heavy_ablang.tokenizer.vocab_to_aa.values())[4:])\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)\n",
    "\n",
    "likelihoods = get_pseudo_likelihood(probs, sequences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724c04-354c-4a43-8c60-1bbc1c7192c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tcrbert = BertModel.from_pretrained(\"wukevin/tcr-bert\")\n",
    "tcrbert_tokenizer = BertTokenizer.from_pretrained(\"wukevin/tcr-bert\")\n",
    "\n",
    "for sequence in tqdm(sequences):\n",
    "    seq_tokens = ' '.join(sequence)\n",
    "    seq_tokens = tcrbert_tokenizer(seq_tokens, return_tensors='pt')\n",
    "    logits = tcrbert(**seq_tokens).logits[0].detach().numpy()\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = tcrbert_tokenizer.vocab)\n",
    "    df = df.iloc[:,5:-5]\n",
    "    df = df.loc[:, df.columns.isin([\"U\",\"Z\",\"O\",\"B\",\"X\"]) == False]\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7768c77c-d980-46ca-b427-6c4c15037398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skorch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class TCRBert():\n",
    "    \"\"\"\n",
    "    Class for the TCRBert Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='average', file_name = 'TCRBert'):\n",
    "        \"\"\"\n",
    "        Creates the instance of the language model instance, loads tokenizer and model\n",
    "        \"\"\"\n",
    "        self.model = BertModel.from_pretrained(\"wukevin/tcr-bert-mlm-only\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"wukevin/tcr-bert-mlm-only\")\n",
    "        self.method = method\n",
    "        self.file = file_name\n",
    "        \n",
    "    def fit_transform(self, sequences:list):\n",
    "        \"\"\"\n",
    "        Fits the model and outputs the embeddings.\n",
    "        parameters\n",
    "        ----------\n",
    "        sequences: `list` \n",
    "        List with sequences to be transformed\n",
    "        ------\n",
    "        None, saved the embeddings in the embeddings.csv\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        print(\"Using '\"+self.method+\"' Method\")\n",
    "        for sequence in sequences:\n",
    "            sequence = ' '.join(sequence)\n",
    "            token = self.tokenizer(sequence, return_tensors=\"pt\")\n",
    "            print(token)\n",
    "            output = self.model(**token)\n",
    "            if self.method == \"average\":\n",
    "                output = torch.mean(output.last_hidden_state, axis = 1)[0]\n",
    "                    \n",
    "            elif self.method == \"pooler\":\n",
    "                output = output.pooler_output[0]\n",
    "                \n",
    "            elif self.method == \"first\":\n",
    "                output = output.last_hidden_state[0,0,:]\n",
    "\n",
    "            elif self.method == \"last\":\n",
    "                output = output.last_hidden_state[0,-1,:]\n",
    "            \n",
    "            embeddings.append(output.tolist())\n",
    "        \n",
    "        pd.DataFrame(embeddings).to_csv(\"outfiles/\"+self.file+\"/embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "009d4b97-f257-4a81-9515-8973acae6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at wukevin/tcr-bert-mlm-only and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tcrbert = TCRBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d278bde3-0db4-48ef-b760-c96be9f1dda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
