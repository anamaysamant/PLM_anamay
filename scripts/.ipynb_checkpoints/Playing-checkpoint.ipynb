{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "176582dc-8392-489e-a0de-b8ae2653a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from utils import get_pseudo_likelihood\n",
    "# Load ESM-2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89cdef0-1dd9-4f9d-8c7c-57a9c5309cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58dc683c-680e-4cab-89af-e23842055c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(67)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(batch_tokens[0] != alphabet.padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdecf1e-d723-436e-b43f-a67b48810218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd7d6d-17f5-41b3-a197-62b4499b1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "             \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\",\n",
    "             \"KALTARQQEVFDLIRDISQTGMPPTRAEIAKALTFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\",\n",
    "             \"KAISQ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d77df-13a8-417b-b414-08a9811c5d57",
   "metadata": {},
   "source": [
    "### ProtBERT Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "599845cd-6220-4dbb-83a0-acc9b89da53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertForMaskedLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import re\n",
    "import protbert\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "def prepare_sequence(sequences):\n",
    "    sequences = [add_space(sequence) for sequence in sequences]\n",
    "    return sequences\n",
    "\n",
    "def add_space(row):\n",
    "    if not isinstance(row, float):\n",
    "        row = \" \".join(row)\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc4ace9b-daeb-4373-a1fa-98b15b463958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 3/4 [01:09<00:23, 23.15s/it]\n"
     ]
    }
   ],
   "source": [
    "sequences = prepare_sequence(sequences)\n",
    "\n",
    "for sequence,_ in zip(enumerate(sequences), tqdm(range(len(sequences)))):\n",
    "            if not isinstance(sequence[1], float):\n",
    "                tokenized_sequences = tokenizer(sequence[1], return_tensors= 'pt') #return tensors using pytorch\n",
    "                output = model(**tokenized_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a117a-7079-4d61-aa6d-058857964f28",
   "metadata": {},
   "source": [
    "### ProtBERT likelihood computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1fdf850-1ef2-4ae1-b1df-167555fae5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "### attempt 1\n",
    "\n",
    "mask_model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "unmasker = pipeline('fill-mask', model=mask_model, tokenizer=tokenizer, top_k = 21)\n",
    "\n",
    "def prepare_sequence_MLM(seq_tokens, pos):\n",
    "    x = seq_tokens.copy()\n",
    "    x[pos] = \"[MASK]\"\n",
    "    return ' '.join(x)\n",
    "\n",
    "probs = []\n",
    "for sequence in tqdm(sequences):\n",
    "    probs_seq = []\n",
    "    seq_tokens = list(sequence)\n",
    "    for pos in tqdm(range(len(sequence))):\n",
    "        prep_seq = prepare_sequence_MLM(seq_tokens, pos)\n",
    "        scores = unmasker(prep_seq)\n",
    "        scores_dict = {dict[\"token_str\"]:dict[\"score\"] for dict in scores}\n",
    "        probs_seq.append(scores_dict)\n",
    "    probs.append(pd.DataFrame(probs_seq))\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9832587-4e47-4f10-86ec-98acc2daa497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### attempt 2\n",
    "\n",
    "import scipy\n",
    "probs = []\n",
    "for sequence in tqdm(sequences):\n",
    "    seq_tokens = ' '.join(list(sequence))\n",
    "    seq_tokens = tokenizer(seq_tokens, return_tensors='pt')\n",
    "    logits = mask_model(**seq_tokens).logits[0].detach().numpy()\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = tokenizer.vocab)\n",
    "    df = df.iloc[:,5:-5]\n",
    "    df = df.loc[:, df.columns.isin([\"U\",\"Z\",\"O\",\"B\",\"X\"]) == False]\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc6ea0-8bf9-4a4b-b7dd-5d94280eb990",
   "metadata": {},
   "source": [
    "### Antiberty likelihood Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9bca5550-ef59-4775-99b1-ab084f06735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [01:28<00:00,  1.37s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [01:51<00:00,  1.57s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:50<00:00,  1.57s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import antiberty\n",
    "\n",
    "antiberty = antiberty.AntiBERTyRunner()\n",
    "\n",
    "pll = antiberty.pseudo_log_likelihood(sequences,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bfe27d6a-ee6f-4062-920e-8af97ef44147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6782, -3.5895, -3.4208, -4.2961])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e557dad-9ea2-4a0c-8f2b-cf4e4f9d4f4c",
   "metadata": {},
   "source": [
    "### Ablang likelihood Computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6c9710cc-a7d5-4cd6-afd7-3c29b42938d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model ...\n"
     ]
    }
   ],
   "source": [
    "import ablang\n",
    "\n",
    "heavy_ablang = ablang.pretrained(\"heavy\")\n",
    "heavy_ablang.freeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a01e4c69-3416-455d-90ee-8764d02051f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "for sequence in tqdm(sequences):\n",
    "    logits = heavy_ablang(sequence, mode=\"likelihood\")[0]\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = list(heavy_ablang.tokenizer.vocab_to_aa.values())[4:])\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)\n",
    "\n",
    "likelihoods = get_pseudo_likelihood(probs, sequences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724c04-354c-4a43-8c60-1bbc1c7192c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tcrbert = BertModel.from_pretrained(\"wukevin/tcr-bert\")\n",
    "tcrbert_tokenizer = BertTokenizer.from_pretrained(\"wukevin/tcr-bert\")\n",
    "\n",
    "for sequence in tqdm(sequences):\n",
    "    seq_tokens = ' '.join(sequence)\n",
    "    seq_tokens = tcrbert_tokenizer(seq_tokens, return_tensors='pt')\n",
    "    logits = tcrbert(**seq_tokens).logits[0].detach().numpy()\n",
    "    prob = scipy.special.softmax(logits,axis = 1)\n",
    "    df = pd.DataFrame(prob, columns = tcrbert_tokenizer.vocab)\n",
    "    df = df.iloc[:,5:-5]\n",
    "    df = df.loc[:, df.columns.isin([\"U\",\"Z\",\"O\",\"B\",\"X\"]) == False]\n",
    "    #removing CLS and SEP\n",
    "    df = df.iloc[1:-1,:]\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    probs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7768c77c-d980-46ca-b427-6c4c15037398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skorch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class TCRBert():\n",
    "    \"\"\"\n",
    "    Class for the TCRBert Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='average', file_name = 'TCRBert'):\n",
    "        \"\"\"\n",
    "        Creates the instance of the language model instance, loads tokenizer and model\n",
    "        \"\"\"\n",
    "        self.model = BertModel.from_pretrained(\"wukevin/tcr-bert-mlm-only\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"wukevin/tcr-bert-mlm-only\")\n",
    "        self.method = method\n",
    "        self.file = file_name\n",
    "        \n",
    "    def fit_transform(self, sequences:list):\n",
    "        \"\"\"\n",
    "        Fits the model and outputs the embeddings.\n",
    "        parameters\n",
    "        ----------\n",
    "        sequences: `list` \n",
    "        List with sequences to be transformed\n",
    "        ------\n",
    "        None, saved the embeddings in the embeddings.csv\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        print(\"Using '\"+self.method+\"' Method\")\n",
    "        for sequence in sequences:\n",
    "            sequence = ' '.join(sequence)\n",
    "            token = self.tokenizer(sequence, return_tensors=\"pt\")\n",
    "            print(token)\n",
    "            output = self.model(**token)\n",
    "            if self.method == \"average\":\n",
    "                output = torch.mean(output.last_hidden_state, axis = 1)[0]\n",
    "                    \n",
    "            elif self.method == \"pooler\":\n",
    "                output = output.pooler_output[0]\n",
    "                \n",
    "            elif self.method == \"first\":\n",
    "                output = output.last_hidden_state[0,0,:]\n",
    "\n",
    "            elif self.method == \"last\":\n",
    "                output = output.last_hidden_state[0,-1,:]\n",
    "            \n",
    "            embeddings.append(output.tolist())\n",
    "        \n",
    "        pd.DataFrame(embeddings).to_csv(\"outfiles/\"+self.file+\"/embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "009d4b97-f257-4a81-9515-8973acae6b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at wukevin/tcr-bert-mlm-only and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tcrbert = TCRBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d278bde3-0db4-48ef-b760-c96be9f1dda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'average' Method\n",
      "{'input_ids': tensor([[25, 17,  2,  6, 14,  0,  8,  4,  0, 16,  2,  5, 15, 14,  0, 15, 16,  4,\n",
      "          0,  5,  2,  4, 12, 14,  5, 11, 13,  8, 16, 13,  4,  4, 16,  5, 14,  5,\n",
      "          0,  8, 14, 15, 14,  8,  3, 15, 13, 19, 16,  0,  5, 16, 11, 19,  7, 15,\n",
      "         14, 13,  6, 12,  0, 11, 19, 14, 16, 13, 11, 11, 24]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (67) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtcrbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[206], line 38\u001b[0m, in \u001b[0;36mTCRBert.fit_transform\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m     36\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(sequence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(token)\n\u001b[1;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     40\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(output\u001b[38;5;241m.\u001b[39mlast_hidden_state, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\plm\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    237\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 238\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    240\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (67) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "tcrbert.fit_transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "38df86b3-d8f5-460c-a4be-7e3dd825b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['xyz']], dtype=object)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"a\":[\"xyz\"]}).to_numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
